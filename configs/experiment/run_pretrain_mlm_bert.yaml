# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: mix_modal_data.yaml
  - override /model: pretrain_mlm_bert_model.yaml
  - override /callbacks: default.yaml
  - override /trainer: ddp.yaml
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["bert", "mlm", "pretrain"]
monitor: val/acc
bert_name_or_path: hfl/chinese-roberta-wwm-ext

test: false

seed: 42

trainer:
  min_epochs: 0
  max_epochs: 20
  devices: 1,2,3,4
  precision: 16
  log_every_n_steps: 50

logger:
  wandb:
    project: pretrain_mlm_bert
    tags: ${tags}

callbacks:
  model_checkpoint:
    monitor: ${monitor}
    mode: min
  early_stopping:
    monitor: ${monitor}
    mode: min
    patience: 3

model:
  lr: 0.0001
  bert_name: ${bert_name_or_path}
  num_warmup_steps: 4000

datamodule:
  tokenizer_name: ${bert_name_or_path}
  val_ratio: 0.1
  mlm: true
  whole_word_mask: true
  train_path: /data/clean_raw_text/all_data_cleaned_with_cn_ref.json
  test_path: /data/clean_raw_text/all_data_cleaned_with_cn_ref.json
  batch_size_per_gpu: 32
